---
title: 深度强化学习笔记
hide: false
math: true
abbrlink: 33517
date: 2025-02-25 20:42:33
index\_img:
banner\_img:
category:
tags:
---
# 未完待续

# 蒙特卡洛
## 近似期望
1. 离散型
$$
E(f(X)) = \sum_{i=1}^{n} f(x_i) p_i
$$
2. 连续型
$$
E(f(X)) = \int f(x)·p(x) dx
$$
用如下方式减少内存开销：
初始化$q_{0}=0$，从$t=1$开始，依次更新$q_t$：
$$
q_t = (1 - 1/t)·q_{t-1} + 1/t·f(x_t)
$$
把$1/t$替换为$\alpha_t$，则有：
$$
q_t = (1 - α_t)·q_{t-1} + α_t·f(x_t)
$$
该式称为Robbinson-Monro算法，其中$\alpha_n$为学习率
## 随机梯度下降
随机梯度下降（Stochastic Gradient Descent, SGD）是一种常用的优化算法，用于最小化目标函数。我们可以用蒙特卡洛方法的思想来理解随机梯度下降。
神经网络的训练可以定义为这样的优化问题：
$$
\min_{w} \mathbb{E}_{X \sim p(·)}[L(X;w)]
$$
其中，$w$是神经网络的参数，$p(·)$是输入数据的分布，$L$是损失函数。
目标函数的梯度\mathbb{E}_{X \sim p(·)}[L(X;w)]关于参数$w$的梯度可以表示为：
$$
g \triangleq \nabla_w \mathbb{E}_{X \sim p(·)}[L(X;w)] = \mathbb{E}_{X \sim p(·)}[\nabla_w L(X;w)]
$$
可以做梯度下降更新w，以减小目标函数的值：
$$
w \leftarrow w - \alpha g
$$
其中，$\alpha$是学习率。直接计算全部样本的梯度g的期望会非常耗时，因此做蒙特卡洛方法近似，把得到的近似梯度$\hat{g}$作为w的更新量。
1. 根据p(x)随机采样得到B个样本，记作$\hat{x}_1, \hat{x}_2, \ldots, \hat{x}_B$
2. 计算每个样本的梯度$\hat{g}_i = \nabla_w L(\hat{x}_i;w)$
3. 计算B个样本的梯度的平均值$\hat{g} = \frac{1}{B} \sum_{i=1}^B \hat{g}_i$
4. 更新w：$w \leftarrow w - \alpha \hat{g}$
样本数量B称作batch size（批大小），通常是比较小的正整数，比如1、8、16、32等。所以称之为小批量SGD。
在实际应用中，p(x)一般未知，训练神经网络时通常收集一个训练集$\mathcal{X} = {x_1, x_2, \ldots, x_n}$，并求解下面这样一个经验风险最小化问题：
$$
\min_{w} \frac{1}{n} \sum_{i=1}^n L(x_i;w)
$$
这相当于用下面这个概率密度函数来代替 $p(x)$：
$$
p(x) = \begin{cases}
\frac{1}{n} & \text{if } x \in \mathcal{X} \\
0 & \text{otherwise}
\end{cases}
$$

# 强化学习基本概念
## 回合episode和epoch的区别
- episode\
回合指示一个完整的交互过程，包括一系列的状态、动作和奖励。一个回合可以包含多个交互步骤，直到达到终止状态或达到最大步数。就是打完一把游戏。
- epoch\
一个epoch表示在整个训练数据集上进行一次完整的前向传播和反向传播过程。一个epoch通常包含多个回合，每个回合都包含多个交互步骤。

## 回报return
return是从当前时刻开始，到回合结束的所有奖励之和，也叫为累计奖励。
强化学习的目标是最大化回报，而不是最大化当前奖励。

## 折扣率$\gamma$
使用折扣率小于1，无限期MDP的回报是有界的。

## 动作价值函数$Q_{\pi}(s,a)$
动作价值函数是指在给定状态$s$和动作$a$的情况下，根据策略$\pi$，未来可能获得的累计奖励的期望值。
$$
Q_{\pi}(s,a) = \mathbb{E}_{\pi}[R_{t+1} + \gamma R_{t+2} + \cdots | S_t = s, A_t = a]
$$

## 最优动作价值函数$Q^*(s_t,a_t)$
最优动作价值函数是指在给定状态$s_t$和动作$a_t$的情况下，在最优策略下未来可能获得的累计奖励的期望值。
$$
Q^*(s_t,a_t) = \max_{a} Q_{\pi}(s_t,a)
$$
意思就是有多种策略$\pi$可以选择，选择最优策略$\pi^*$，在最优策略$\pi^*$下未来可能获得的累计奖励的期望值。

## 状态价值函数$V_{\pi}(s)$
状态价值函数是指根据策略$\pi$,在给定状态$s$的情况下，未来可能获得的累计奖励的期望值。
$$
V_{\pi}(s_t) = \mathbb{E}_{\pi}[Q_{\pi}(s_t,a_t) | S_t = s_t]
= \sum_{a \in \mathcal{A}} \pi(a|s_t) Q_{\pi}(s_t,a)
$$
只依赖于状态$s_t$，不依赖于动作$a_t$。也是回报的期望值。

# DQN与Q-Learning
用$Q^*$最优动作价值函数使用最大化消除策略$\pi$。\
可以这样理解$Q^*$：已知状态$s_t$和动作$a_t$，不论未来采取什么样的策略$\pi$，回报的期望不可能超过$Q^*$。
所以要训练一个近似的$Q^*$。
## 深度Q网络（Deep Q Network, DQN）
记作Q(s,a;w)，其结构如下：
1. 输入：状态$s$
2. 卷积层：提取特征
3. 全连接层：映射到动作空间
4. 输出：离散动作空间$\mathcal{A}$上的每个动作$a$的Q值，即输出是一个$\mathcal{A}$维的向量

**DQN的梯度：**
$$
\nabla_w Q(s,a;w) \triangleq \frac{\partial Q(s,a;w)}{\partial w}
$$
### TD算法
训练DQN最常用的算法是TD（Temporal Difference, 时序差分）算法。
**损失函数：**
$$
L(w) =  \frac{1}{2}[Q(s,a;w) - y]^2
$$
**梯度：**
$$
\nabla_w L(w) = \nabla_w  \frac{1}{2} [Q(s,a;w) - y]^2 = (\hat{q} - y)·\nabla_w Q(s,a;w)
$$
然后梯度下降更新w。
TD算法就是把预测到中途的数据也利用上。
把到中途的q记作r（训练时是一个状态上的奖励），中途之后的路程$\hat{q}$，则TD目标为$\hat{y} \triangleq r + \hat{q}$，新的梯度为：
$$
\nabla_w L(w) = (\hat{q} - \hat{y})·\nabla_w Q(s,a;w)
$$
$\delta = \hat{q} - \hat{y}$ 称为TD误差。做一次梯度下降更新w：
$$
w \leftarrow w - \alpha \nabla_w L(w) = w - \alpha (\hat{q} - \hat{y})·\nabla_w Q(s,a;w) = w - \alpha · \delta ·\nabla_w Q(s,a;w)
$$
TD误差$\delta$就是模型的估计与真实值之间的差距。

### 用TD算法训练DQN
近似可得：
$$
Q^*(s_t,a_t) \approx r_t + \gamma · \max_{a \in \mathcal{A}}Q^*(s_{t+1},a)
$$
把上式中的$Q^*(·)$替换为神经网络$Q(s,a;w)$，则有：
$$
Q(s,a;w) \approx r_t + \gamma · \max_{a \in \mathcal{A}}Q(s_{t+1},a;w)
$$
做梯度下降：
$$
w \leftarrow w - \alpha · \delta_{t} ·\nabla_w Q(s_t,a_t;w)
$$

### 训练流程
**收集训练数据**
用任意策略$\pi$与环境交互，收集训练数据，此时称为行为策略$\pi$。比较常用的是$\epsilon$-贪婪策略。
$$
a_t = \begin{cases}
argmax_{a} Q(s_t,a;w) & \text{with probability } 1 - \epsilon \\
\mathcal{A} 中的随机动作 & \text{with probability } \epsilon
\end{cases}
$$
把一个回合的数据称为一个轨迹，记作：
$$
\mathcal{T} = \{s_1,a_1,r_1,s_2,a_2,r_2,\cdots,s_T,a_T,r_T\}
$$
把一条轨迹分为n个$(s_t,a_t,r_t,s_{t+1})$这样的四元组，存入缓存，这个缓存称为回放缓冲区。

**更新参数w**
从经验回放缓冲区中随机采样一个小批量的四元组$(s_t,a_t,r_t,s_{t+1})$，当前参数$w_{now}$，执行以下步骤对参数做一次更新，得到新参数$w_{new}$：
1. 对DQN做正向传播，得到Q值：
$$
\hat{q}_j = Q(s_j,a_j;w_{now})
$$
$$
\hat{q}_{j+1} = \max_{a \in \mathcal{A}} Q(s_{j+1},a;w_{now})
$$
2. 计算TD目标和TD误差：
$$
\hat{y}_j = r_j + \gamma · \hat{q}_{j+1}
$$
$$
\delta_j = \hat{q}_j - \hat{y}_j
$$
3. 对DQN进行反向传播，计算梯度：
$$
g_j = \nabla_w Q(s_j,a_j;w_{now})
$$
4. 梯度下降更新参数：
$$
w_{new} = w_{now} - \alpha · \delta_j · g_j
$$
收集数据和更新参数可以同时进行。

## Q-Learning
### TD目标
$$
\hat{y}_j = r_j + \gamma · \max_{a \in \mathcal{A}} \overset{\sim}{Q}(s_{j+1},a)
$$
### 更新表格$\overset{\sim}{Q}$中$(s_t,a_t)$位置上的元素：
$$
\overset{\sim}{Q}(s_t,a_t) \leftarrow \overset{\sim}{Q}(s_t,a_t) + \alpha · (\hat{y}_j - \overset{\sim}{Q}(s_t,a_t))
$$
### 训练流程
**收集训练数据**
与DQN一致
**经验回放更新表格$\overset{\sim}{Q}$**
随机从经验回放缓冲区中采样一个小批量的四元组$(s_j,a_j,r_j,s_{j+1})$，记作$Q_{now}$，执行以下步骤对表格$\overset{\sim}{Q}$做一次更新：
1. 把$Q_{now}$中$$(s_j,a_j)$$位置上的元素记作$q_j$:
$$
q_j = \overset{\sim}{Q}(s_j,a_j)
$$
2. 把第$s_{j+1}$行的最大Q值记作$\hat{q}_{j+1}$：
$$
\hat{q}_{j+1} = \max_{a \in \mathcal{A}} \overset{\sim}{Q}_{now}(s_{j+1},a)
$$
3. 计算TD目标和TD误差：
$$
\hat{y}_j = r_j + \gamma · \hat{q}_{j+1}
$$
$$
\delta_j = \hat{q}_j - \hat{y}_j
$$
4. 更新表格$\overset{\sim}{Q}$中$(s_j,a_j)$位置上的元素：
$$
\overset{\sim}{Q}_{new}(s_j,a_j) \leftarrow \overset{\sim}{Q}_{now}(s_j,a_j) - \alpha · \delta_j
$$

## 同策略与异策略
**同策略（on-policy）**
收集经验的行为策略和更新参数的策略是同一个策略。

**异策略（off-policy）**
收集经验的行为策略和更新参数的策略是不同的策略。
经验。经验回放只适用于异策略。DQN与Q-Learning都是异策略。

# SARSA算法
## 表格形式的SARSA
学习动作价值函数$Q_{\pi}(s,a)$\
TD目标：
$$
\hat{y}_j \triangleq r_j + \gamma · q (s_{j+1},\overset{\sim}{a}_{j+1})
$$
更新表格$s$中的$(s_t,a_t)$位置上的元素：
$$
q(s_t,a_t) \leftarrow q(s_t,a_t) + \alpha · (\hat{y}_t - q(s_t,a_t))
$$

**训练流程**\
设当前表格$q$，当前策略为${\pi}_{now}$,每一轮更新表格中的一个元素，把更新之后的表格记为$q_{new}$，执行以下步骤：
1. 观测到当前状态$s_t$，根据当前策略做抽样：$$a_t \sim \pi_{now}(·|s_t)$$
2. 把$q_{now}$中$$(s_t,a_t)$$位置上的元素记作：
$$
\hat{q}_t = q_{now}(s_t,a_t)
$$
3. 执行动作$a_t$，得到奖励$r_t$和下一个状态$s_{t+1}$
4. 根据当前策略做抽样：$\overset{\sim}{a}_{t+1} \sim \pi_{now}(·|s_{t+1})$，$\overset{\sim}{a}_{t+1}$并不执行，是假想的动作。
5. 把$q_{now}$中$$(s_{t+1},\overset{\sim}{a}_{t+1})$$位置上的元素记作：
$$
\hat{q}_{t+1} = q_{now}(s_{t+1},\overset{\sim}{a}_{t+1})
$$
6. 计算TD目标和TD误差：
$$
\hat{y}_t = r_t + \gamma · \hat{q}_{t+1}
$$
$$
\delta_t = \hat{q}_t - \hat{y}_t
$$
7. 更新表格$q_{new}$中$(s_t,a_t)$位置上的元素：
$$
q_{new}(s_t,a_t) \leftarrow q_{now}(s_t,a_t) - \alpha · \delta_t
$$
8. 用某种算法更新策略$\pi_{new}$。该算法与SARSA算法无关。

SARSA算法是同策略的，近似$Q_{\pi}$。\
Q学习算法是异策略的，近似$Q^*$。

## 神经网络形式的SARSA
用神经网络$q(s,a;w)$来近似$Q_{\pi}(s,a)$，也称为价值网络，首先随机初始化w，然后用SARSA算法更新w。

**梯度下降**
$$
w \leftarrow w - \alpha · \delta_t · \nabla_w q(s_t,a_t;w)
$$

**训练流程**
1. 观测到当前状态$s_t$，根据当前策略做抽样：$$a_t \sim \pi_{now}(·|s_t)$$
2. 用神经网络$q(s_t,a_t;w)$做正向传播，得到Q值：
$$
\hat{q}_t = q(s_t,a_t;w_{now})
$$
3. 执行动作$a_t$，得到奖励$r_t$和下一个状态$s_{t+1}$
4. 根据当前策略做抽样：$\overset{\sim}{a}_{t+1} \sim \pi_{now}(·|s_{t+1})$，$\overset{\sim}{a}_{t+1}$并不执行，是假想的动作。
5. 用神经网络$q(s_{t+1},\overset{\sim}{a}_{t+1};w)$做正向传播，得到Q值：
$$
\hat{q}_{t+1} = q(s_{t+1},\overset{\sim}{a}_{t+1};w)
$$
6. 计算TD目标和TD误差：
$$
\hat{y}_t = r_t + \gamma · \hat{q}_{t+1}
$$
$$
\delta_t = \hat{q}_t - \hat{y}_t
$$
7. 用神经网络$q(s_t,a_t;w)$做反向传播，计算梯度：
$$
g_t = \nabla_w q(s_t,a_t;w)
$$
8. 梯度下降更新参数：
$$
w_{new} = w_{now} - \alpha · \delta_t · g_t
$$
9. 用某种算法更新策略$\pi_{new}$。该算法与SARSA算法无关。

## 多步TD目标
m步TD目标（到中途前走了m步）：
$$
\hat{y}_t \triangleq \sum_{k=0}^{m-1} \gamma^k r_{t+k} + \gamma^m · \hat{q}_{t+m}
$$

**训练流程**
需要注意的是更新价值网络参数（$(n-m)$表示回到中途需要这么多步）：
$$
w \leftarrow w - \alpha · \sum_{k=0}^{n-m} \gamma^k \delta_t · g_t
$$

## 蒙特卡洛方法与自举
**蒙特卡洛方法**
用实际观测值近似期望。
好处：无偏性。
坏处：方差大。

**自举（bootstraping）**
TD目标。
好处：方差小，收敛快。
坏处：有偏差，会让偏差从$(s_{t+1},a_{t+1})$传播到$(s_t,a_t)$。

# 价值学习高级技巧
## 经验回放
把智能体与环境交互的记录（即经验）存储在经验回放缓存中。实践中要等待经验回放缓存中有足够多的四元组才开始做经验回放更新DQN。

**经验回放的优点**
1. 收集经验时相邻两个四元组有很强的相关性，经验回放每次从缓存里随机抽取一个四元组，可以打破相关性，提高训练效率。
2. 重复利用收集到的经验，可以用更少的样本数量达到同样的效果。

**局限性**
不适用于on-policy算法。

**优先经验回放**
优先经验回放是指在经验回放时，根据经验的重要性(TD误差大的)，给经验赋予不同的权重，从而使得重要经验被重复利用的概率更大。做非均匀抽样会导致DQN的预测有偏差。应该根据抽样概率调整学习率：
$$
\alpha_j = \alpha · \frac{1}{(b · p_{j})^\beta}
$$
b是经验回放缓存中样本的总数，$\beta \in (0,1)$是一个需要调整的超参数。

## 高估问题及解决方法
Q学习算法训练出来的DQN会高估真实的价值：
1. 自举导致偏差传播。
2. 最大化导致高估。

想要解决高估，要么**切断自举**，要么**避免最大化**。

**使用目标网络**
使用目标网络可以切断自举。
用另一个神经网络，即目标网络来计算TD目标，记作：
$$
Q(s,a;w^-)
$$
设DQN和目标网络当前的参数分别为$w_{now}$和$w_{now}^-$，执行以下步骤：
1. 对DQN做正向传播，得到:
$$
\hat{q}_t = Q(s_t,a_t;w_{now})
$$
2. 对目标网络做正向传播，得到：
$$
\hat{q}_{t+1}^- = \max_{a \in \mathcal{A}} Q(s_{t+1},a;w_{now}^-)
$$
3. 计算TD目标和TD误差：
$$
\hat{y}_t = r_t + \gamma · \hat{q}_{t+1}^-
$$
$$
\delta_t = \hat{q}_t - \hat{y}_t^-
$$
4. 对DQN做反向传播，计算梯度：
$$
g_t = \nabla_w Q(s_t,a_t;w_{now})
$$
5. 梯度下降更新参数：
$$
w_{new} = w_{now} - \alpha · \delta_t · g_t
$$
6. 设$\tau \in (0,1)$是需要手动调节的超参数，做加权平均：
$$
w_{neew}^- \leftarrow \tau · w_{new} + (1-\tau) · w_{now}^-
$$
这种方法只是减轻自举，不能完全避免自举。

**使用双Q网络**
使用双Q网络可以避免最大化。
双Q学习算法，也就是double DQN，缩写DDQN。

**训练流程**
1. 对DQN做正向传播，得到：
$$
\hat{q}_t = Q(s_t,a_t;w_{now})
$$
2. 选择：
$$
a^* = \argmax_{a \in \mathcal{A}} Q(s_{t+1},a;w_{now})
$$
3. 求值：
$$
\hat{q}_{t+1} = Q(s_{t+1},a^*;w_{now}^-)
$$
4. 计算TD目标和TD误差：
$$
\overset{\sim}{y}_t = r_t + \gamma · \hat{q}_{t+1}
$$
$$
\delta_t = \hat{q}_t - \overset{\sim}{y}_t
$$
5. 对DQN做反向传播，计算梯度：
$$
g_t = \nabla_w Q(s_t,a_t;w_{now})
$$
6. 梯度下降更新参数：
$$
w_{new} = w_{now} - \alpha · \delta_t · g_t
$$
7. 设$\tau \in (0,1)$是需要手动调节的超参数，做加权平均：
$$
w_{new}^- \leftarrow \tau · w_{new} + (1-\tau) · w_{now}^-
$$

## 对决网络（dueling network）
**最优优势函数**
定义：
$$
D^*(s,a) = Q^*(s,a) - V^*(s)
$$

**dueling network结构**
