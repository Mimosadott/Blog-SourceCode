---
title: 深度强化学习笔记
hide: false
math: true
abbrlink: 33517
date: 2025-02-25 20:42:33
index\_img:
banner\_img:
category:
tags:
---
# 未完待续

# 蒙特卡洛
## 近似期望
1. 离散型
$$
E(f(X)) = \sum_{i=1}^{n} f(x_i) p_i
$$
2. 连续型
$$
E(f(X)) = \int f(x)·p(x) dx
$$
用如下方式减少内存开销：
初始化$q_{0}=0$，从$t=1$开始，依次更新$q_t$：
$$
q_t = (1 - 1/t)·q_{t-1} + 1/t·f(x_t)
$$
把$1/t$替换为$\alpha_t$，则有：
$$
q_t = (1 - α_t)·q_{t-1} + α_t·f(x_t)
$$
该式称为Robbinson-Monro算法，其中$\alpha_n$为学习率
## 随机梯度下降
随机梯度下降（Stochastic Gradient Descent, SGD）是一种常用的优化算法，用于最小化目标函数。我们可以用蒙特卡洛方法的思想来理解随机梯度下降。
神经网络的训练可以定义为这样的优化问题：
$$
\min_{w} \mathbb{E}_{X \sim p(·)}[L(X;w)]
$$
其中，$w$是神经网络的参数，$p(·)$是输入数据的分布，$L$是损失函数。
目标函数的梯度\mathbb{E}_{X \sim p(·)}[L(X;w)]关于参数$w$的梯度可以表示为：
$$
g \triangleq \nabla_w \mathbb{E}_{X \sim p(·)}[L(X;w)] = \mathbb{E}_{X \sim p(·)}[\nabla_w L(X;w)]
$$
可以做梯度下降更新w，以减小目标函数的值：
$$
w \leftarrow w - \alpha g
$$
其中，$\alpha$是学习率。直接计算全部样本的梯度g的期望会非常耗时，因此做蒙特卡洛方法近似，把得到的近似梯度$\hat{g}$作为w的更新量。
1. 根据p(x)随机采样得到B个样本，记作$\hat{x}_1, \hat{x}_2, \ldots, \hat{x}_B$
2. 计算每个样本的梯度$\hat{g}_i = \nabla_w L(\hat{x}_i;w)$
3. 计算B个样本的梯度的平均值$\hat{g} = \frac{1}{B} \sum_{i=1}^B \hat{g}_i$
4. 更新w：$w \leftarrow w - \alpha \hat{g}$
样本数量B称作batch size（批大小），通常是比较小的正整数，比如1、8、16、32等。所以称之为小批量SGD。
在实际应用中，p(x)一般未知，训练神经网络时通常收集一个训练集$\mathcal{X} = {x_1, x_2, \ldots, x_n}$，并求解下面这样一个经验风险最小化问题：
$$
\min_{w} \frac{1}{n} \sum_{i=1}^n L(x_i;w)
$$
这相当于用下面这个概率密度函数来代替 $p(x)$：
$$
p(x) = \begin{cases}
\frac{1}{n} & \text{if } x \in \mathcal{X} \\
0 & \text{otherwise}
\end{cases}
$$

# 强化学习基本概念
## 回合episode和epoch的区别
- episode\
回合指示一个完整的交互过程，包括一系列的状态、动作和奖励。一个回合可以包含多个交互步骤，直到达到终止状态或达到最大步数。就是打完一把游戏。
- epoch\
一个epoch表示在整个训练数据集上进行一次完整的前向传播和反向传播过程。一个epoch通常包含多个回合，每个回合都包含多个交互步骤。

## 回报return
return是从当前时刻开始，到回合结束的所有奖励之和，也叫为累计奖励。
强化学习的目标是最大化回报，而不是最大化当前奖励。

## 折扣率$\gamma$
使用折扣率小于1，无限期MDP的回报是有界的。

## 动作价值函数$Q_{\pi}(s,a)$
动作价值函数是指在给定状态$s$和动作$a$的情况下，根据策略$\pi$，未来可能获得的累计奖励的期望值。
$$
Q_{\pi}(s,a) = \mathbb{E}_{\pi}[R_{t+1} + \gamma R_{t+2} + \cdots | S_t = s, A_t = a]
$$

## 最优动作价值函数$Q^*(s_t,a_t)$
最优动作价值函数是指在给定状态$s_t$和动作$a_t$的情况下，在最优策略下未来可能获得的累计奖励的期望值。
$$
Q^*(s_t,a_t) = \max_{a} Q_{\pi}(s_t,a)
$$
意思就是有多种策略$\pi$可以选择，选择最优策略$\pi^*$，在最优策略$\pi^*$下未来可能获得的累计奖励的期望值。

## 状态价值函数$V_{\pi}(s)$
状态价值函数是指根据策略$\pi$,在给定状态$s$的情况下，未来可能获得的累计奖励的期望值。
$$
V_{\pi}(s_t) = \mathbb{E}_{\pi}[Q_{\pi}(s_t,a_t) | S_t = s_t]
= \sum_{a \in \mathcal{A}} \pi(a|s_t) Q_{\pi}(s_t,a)
$$
只依赖于状态$s_t$，不依赖于动作$a_t$。也是回报的期望值。

# DQN与Q-Learning
用$Q^*$最优动作价值函数使用最大化消除策略$\pi$。\
可以这样理解$Q^*$：已知状态$s_t$和动作$a_t$，不论未来采取什么样的策略$\pi$，回报的期望不可能超过$Q^*$。
所以要训练一个近似的$Q^*$。
## 深度Q网络（Deep Q Network, DQN）
记作Q(s,a;w)，其结构如下：
1. 输入：状态$s$
2. 卷积层：提取特征
3. 全连接层：映射到动作空间
4. 输出：离散动作空间$\mathcal{A}$上的每个动作$a$的Q值，即输出是一个$\mathcal{A}$维的向量

**DQN的梯度：**
$$
\nabla_w Q(s,a;w) \triangleq \frac{\partial Q(s,a;w)}{\partial w}
$$
### TD算法
训练DQN最常用的算法是TD（Temporal Difference, 时序差分）算法。
**损失函数：**
$$
L(w) =  \frac{1}{2}[Q(s,a;w) - y]^2
$$
**梯度：**
$$
\nabla_w L(w) = \nabla_w  \frac{1}{2} [Q(s,a;w) - y]^2 = (\hat{q} - y)·\nabla_w Q(s,a;w)
$$
然后梯度下降更新w。
TD算法就是把预测到中途的数据也利用上。
把到中途的q记作r（训练时是一个状态上的奖励），中途之后的路程$\hat{q}$，则TD目标为$\hat{y} \triangleq r + \hat{q}$，新的梯度为：
$$
\nabla_w L(w) = (\hat{q} - \hat{y})·\nabla_w Q(s,a;w)
$$
$\delta = \hat{q} - \hat{y}$ 称为TD误差。做一次梯度下降更新w：
$$
w \leftarrow w - \alpha \nabla_w L(w) = w - \alpha (\hat{q} - \hat{y})·\nabla_w Q(s,a;w) = w - \alpha · \delta ·\nabla_w Q(s,a;w)
$$
TD误差$\delta$就是模型的估计与真实值之间的差距。

### 用TD算法训练DQN
近似可得：
$$
Q^*(s_t,a_t) \approx r_t + \gamma · \max_{a \in \mathcal{A}}Q^*(s_{t+1},a)
$$
把上式中的$Q^*(·)$替换为神经网络$Q(s,a;w)$，则有：
$$
Q(s,a;w) \approx r_t + \gamma · \max_{a \in \mathcal{A}}Q(s_{t+1},a;w)
$$
做梯度下降：
$$
w \leftarrow w - \alpha · \delta_{t} ·\nabla_w Q(s_t,a_t;w)
$$

### 训练流程
**收集训练数据**
用任意策略$\pi$与环境交互，收集训练数据，此时称为行为策略$\pi$。比较常用的是$\epsilon$-贪婪策略。
$$
a_t = \begin{cases}
argmax_{a} Q(s_t,a;w) & \text{with probability } 1 - \epsilon \\
\mathcal{A} 中的随机动作 & \text{with probability } \epsilon
\end{cases}
$$
把一个回合的数据称为一个轨迹，记作：
$$
\mathcal{T} = \{s_1,a_1,r_1,s_2,a_2,r_2,\cdots,s_T,a_T,r_T\}
$$
把一条轨迹分为n个$(s_t,a_t,r_t,s_{t+1})$这样的四元组，存入缓存，这个缓存称为回放缓冲区。

**更新参数w**
从经验回放缓冲区中随机采样一个小批量的四元组$(s_t,a_t,r_t,s_{t+1})$，当前参数$w_{now}$，执行以下步骤对参数做一次更新，得到新参数$w_{new}$：
1. 对DQN做正向传播，得到Q值：
$$
\hat{q}_j = Q(s_j,a_j;w_{now})
$$
$$
\hat{q}_{j+1} = \max_{a \in \mathcal{A}} Q(s_{j+1},a;w_{now})
$$
2. 计算TD目标和TD误差：
$$
\hat{y}_j = r_j + \gamma · \hat{q}_{j+1}
$$
$$
\delta_j = \hat{q}_j - \hat{y}_j
$$
3. 对DQN进行反向传播，计算梯度：
$$
g_j = \nabla_w Q(s_j,a_j;w_{now})
$$
4. 梯度下降更新参数：
$$
w_{new} = w_{now} - \alpha · \delta_j · g_j
$$
收集数据和更新参数可以同时进行。

## Q-Learning
### TD目标
$$
\hat{y}_j = r_j + \gamma · \max_{a \in \mathcal{A}} \overset{\sim}{Q}(s_{j+1},a)
$$
### 更新表格$\overset{\sim}{Q}$中$(s_t,a_t)$位置上的元素：
$$
\overset{\sim}{Q}(s_t,a_t) \leftarrow \overset{\sim}{Q}(s_t,a_t) + \alpha · (\hat{y}_j - \overset{\sim}{Q}(s_t,a_t))
$$
### 训练流程
**收集训练数据**
与DQN一致
**经验回放更新表格$\overset{\sim}{Q}$**
随机从经验回放缓冲区中采样一个小批量的四元组$(s_j,a_j,r_j,s_{j+1})$，记作$Q_{now}$，执行以下步骤对表格$\overset{\sim}{Q}$做一次更新：
1. 把$Q_{now}$中$$(s_j,a_j)$$位置上的元素记作$q_j$:
$$
q_j = \overset{\sim}{Q}(s_j,a_j)
$$
2. 把第$s_{j+1}$行的最大Q值记作$\hat{q}_{j+1}$：
$$
\hat{q}_{j+1} = \max_{a \in \mathcal{A}} \overset{\sim}{Q}_{now}(s_{j+1},a)
$$
3. 计算TD目标和TD误差：
$$
\hat{y}_j = r_j + \gamma · \hat{q}_{j+1}
$$
$$
\delta_j = \hat{q}_j - \hat{y}_j
$$
4. 更新表格$\overset{\sim}{Q}$中$(s_j,a_j)$位置上的元素：
$$
\overset{\sim}{Q}_{new}(s_j,a_j) \leftarrow \overset{\sim}{Q}_{now}(s_j,a_j) - \alpha · \delta_j
$$

## 同策略与异策略
**同策略（on-policy）**
收集经验的行为策略和更新参数的策略是同一个策略。

**异策略（off-policy）**
收集经验的行为策略和更新参数的策略是不同的策略。
经验。经验回放只适用于异策略。DQN与Q-Learning都是异策略。

# SARSA算法
